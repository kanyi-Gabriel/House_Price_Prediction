{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# House Price Prediction & Analysis\n",
    "\n",
    "The ojective is to understand key drivrs of house prices of houses and build a Predictive model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### 1. Set up & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading the data\n",
    "def wrangle(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train and the test data\n",
    "df_train = wrangle(r\"C:\\Users\\User\\Desktop\\Completed\\House_Price_Prediction\\project_files\\train.csv\")\n",
    "df_test = wrangle(r\"C:\\Users\\User\\Desktop\\Completed\\House_Price_Prediction\\project_files\\test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### EDA: Distribution and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 column of the train dataset\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap of the data\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To exploreinformation about data\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values\n",
    "missing = df_train.isna().mean().sort_values(ascending = False)\n",
    "missing = missing[missing > 0] * 100\n",
    "missing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topmost missing values\n",
    "top_missing = missing.head(20)\n",
    "plt.figure(figsize = (6, 4))\n",
    "top_missing.sort_values().plot(kind = \"barh\")\n",
    "\n",
    "# Label Axis\n",
    "plt.xlabel(\"Missing Frequency\")\n",
    "plt.ylabel(\"Features\")\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Distribution of Missing Values\")\n",
    "plt.tight_layout()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of the target with numerical features\n",
    "num_cols = df_train.select_dtypes(include=[np.number])\n",
    "corr = num_cols.corr().drop(\"SalePrice\")\n",
    "#  Visualie correlation with a heatmap\n",
    "plt.figure(figsize = [8,6])\n",
    "sns.heatmap(corr, annot = False, vmin=-1, center = 0, vmax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuaize top correlated\n",
    "top_corr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Preprocessing Function \n",
    "def preprocessing(df):\n",
    "    # Keep the original training dataset\n",
    "    df_preprocess = df_train.copy()\n",
    "    # Droping colums with alot missing values\n",
    "    df_preprocess.drop(columns=[\"Id\", \"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"MasVnrType\",\"FireplaceQu\"], inplace =True)\n",
    "\n",
    "    #Dropig multicolinearlity columns\n",
    "    df_preprocess.drop(columns=[\"LotArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"GrLivArea\"], inplace = True)\n",
    "    \n",
    "    return df_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = preprocessing(df_train)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ta = df_train.nunique()\n",
    "# Export DataFrame into a csv\n",
    "df_ta.to_csv('train unique.csv')\n",
    "\n",
    "# Read the csv file \n",
    "z = pd.read_csv('train unique.csv')\n",
    "\n",
    "# Filter data that have missing values in colmn zero\n",
    "z[z['0'] > 700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print descriptive statistics\n",
    "print(df_train[\"SalePrice\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create the histogram with KDE\n",
    "sns.histplot(df_clean[\"SalePrice\"], color=\"c\", bins=50, kde=True, ax=ax, alpha=0.8)\n",
    "\n",
    "# Adding a title\n",
    "plt.title(\"Distribution of prices of different Houses\");\n",
    "\n",
    "#Labelling the axis\n",
    "plt.ylabel(\"Frequency\");\n",
    "plt.xlabel(\"Price\");\n",
    "\n",
    "# Show the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create axis\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create a plot\n",
    "plt.plot(df_clean[\"SalePrice\"], color=\"c\")\n",
    "\n",
    "# Adding a title\n",
    "\n",
    "plt.title(\"Distributio of prices of different Houses\");\n",
    "\n",
    "#Labelling the axis\n",
    "plt.ylabel(\"Price of the Houses\");\n",
    "plt.xlabel(\"Frequency\");\n",
    "\n",
    "# Show the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the null values in the target'SalePrice'\n",
    "df_train.dropna(axis=0, subset=['SalePrice'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of the numerical data\n",
    "df_num.hist(figsize=(16,20),bins=50, xlabelsize=8, ylabelsize=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepparating the numerical values from the dataset\n",
    "df_num = df_train.select_dtypes(include =[\"float64\", \"int64\"])\n",
    "df_cate = df_train.select_dtypes(include = [\"object\"])\n",
    "\n",
    "# impute missing numerical values\n",
    "imputer_nume = SimpleImputer(strategy='median')\n",
    "df_imputed_nume = pd.DataFrame(imputer_nume.fit_transform(df_num), columns=df_num.columns)\n",
    "\n",
    "# Impute missing categorical values\n",
    "imputer_categ = SimpleImputer(strategy = \"most_frequent\")\n",
    "df_imputed_categ = pd.DataFrame(imputer_categ.fit_transform(df_cate), columns=df_cate.columns)\n",
    "\n",
    "# To concat the imputed datasets\n",
    "df_imputed = pd.concat([df_imputed_nume, df_imputed_categ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To define eature and target\n",
    "y = df_imputed[\"SalePrice\"]\n",
    "X = df_imputed.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To splt data into testing and training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression mode\n",
    "model_lin = LinearRegression()\n",
    "\n",
    "# To train the model using the training data\n",
    "model_lin.fit(X_train, y_train)\n",
    "\n",
    "# To make predictions using liner regression model\n",
    "y_pred = model_lin.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Mean absolute error using Linear Regression:{round(mean_absolute_error(y_test, y_pred),2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree model\n",
    "model_2 = DecisionTreeRegressor()\n",
    "\n",
    "# To train the model using the training data\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "# To make predictions using liner regression model\n",
    "y_pred_1 = model_2.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Mean absolute error using Decision Tree: {round(mean_absolute_error(y_test, y_pred1),2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "model_3 = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "\n",
    "# To train the model using the training data\n",
    "model_3.fit(X_train, y_train)\n",
    "\n",
    "# To make predictions using liner regression model\n",
    "y_pred_2 = model_3.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Mean absolute error using Random forest: {round(mean_absolute_error(y_test, y_pred_2),2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
