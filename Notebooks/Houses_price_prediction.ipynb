{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# House Price Prediction & Analysis\n",
    "\n",
    "The ojective is to understand key drivrs of house prices of houses and build a Predictive model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### 1. Set up & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling Libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor\n",
    "\n",
    "# Utils\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading the data\n",
    "def wrangle(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train and the test data\n",
    "df = wrangle(r\"C:\\Users\\User\\Desktop\\Completed\\House_Price_Prediction\\project_files\\train.csv\")\n",
    "df_test = wrangle(r\"C:\\Users\\User\\Desktop\\Completed\\House_Price_Prediction\\project_files\\test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Targets and Basic Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the target is in the dataset\n",
    "assert \"SalePrice\" in df.columns, \"Expected, column 'SalePrice' as the target\"\n",
    "\n",
    "# Set the target\n",
    "target = \"SalePrice\"\n",
    "\n",
    "# Sepparate feature and target\n",
    "features = [col for col in df.columns if  col not in (\"SalePrice\", \"Id\")]\n",
    "X = df[features]\n",
    "y = df[target].copy()\n",
    "\n",
    "# Categorize numerical features and categorical Features\n",
    "num_cols = X.select_dtypes(include = [np.number]).columns\n",
    "cat_cols = X.select_dtypes(exclude = [np.number]).columns\n",
    "\n",
    "print(f\"Num rows: {len(df)}, Num features: {X.shape[1]}\")\n",
    "print(f\"Numeric features: {len(num_cols)} | Categorical features: {len(cat_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### EDA: Distribution and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 column of the train dataset\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To explore information about data\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values\n",
    "missing = df.isna().mean().sort_values(ascending = False)\n",
    "missing = missing[missing > 0] * 100\n",
    "missing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topmost missing values\n",
    "top_missing = missing.head(20)\n",
    "plt.figure(figsize = (6, 4))\n",
    "top_missing.sort_values().plot(kind = \"barh\")\n",
    "\n",
    "# Label Axis\n",
    "plt.xlabel(\"Missing Frequency\")\n",
    "plt.ylabel(\"Features\")\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Distribution of Missing Values\")\n",
    "plt.tight_layout()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of the features with numerical features\n",
    "num_cols = X.select_dtypes(include=[np.number])\n",
    "corr = num_cols.corr()\n",
    "#  Visualie correlation with a heatmap\n",
    "plt.figure(figsize = [8,6])\n",
    "sns.heatmap(corr, annot = False, vmin=-1, center = 0, vmax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations with target (numeric only)\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corrs = df[numeric_cols + [target]].corr(numeric_only=True)[target].drop(target).sort_values(ascending=False)\n",
    "corrs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlated features\n",
    "top_correlated = corrs.head(15)\n",
    "\n",
    "# Bar Plot\n",
    "plt.figure(figsize=(6,4))\n",
    "top_correlated.sort_values().plot(kind=\"barh\")\n",
    "\n",
    "# label Axis\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Feature\")\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Top correlated Numeric features to House Saleprice\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot of Key numerical features to sales\n",
    "key_numeric = [col for col in \n",
    "    [\"OverallQual\",\"GrLivArea\", \n",
    "     \"GarageCars\", \"GarageArea\", \n",
    "     \"TotalBsmtSF\", \"1stFlrSF\",\n",
    "     \"FullBath \", \"TotRmsAbvGrd\",\n",
    "     \"YearBuilt\", \"YearRemodAdd\"] if col in df.columns\n",
    "]  \n",
    "\n",
    "for cols in key_numeric:\n",
    "    plt.figure(figsize = (6,4))\n",
    "    plt.scatter(df[cols], y)\n",
    "    \n",
    "    # Add a title\n",
    "    plt.title(f\" {cols} vs Sales Price\")\n",
    "    \n",
    "    # Label Axis\n",
    "    plt.xlabel(cols)\n",
    "    plt.ylabel(\"Sales Price\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create the histogram with KDE\n",
    "sns.histplot(df[\"SalePrice\"], color=\"c\", bins=50, kde=True, ax=ax, alpha=0.8)\n",
    "\n",
    "# Adding a title\n",
    "plt.title(\"Distribution of prices of different Houses\");\n",
    "\n",
    "#Labelling the axis\n",
    "plt.ylabel(\"Frequency\");\n",
    "plt.xlabel(\"Price\");\n",
    "\n",
    "# Show the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The distribution of prices of house shows **skwness toward the right**. To make the distribution more normal, log function will be apllied during model evaluation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep original copy of the dataset\n",
    "X_feature = X.copy()\n",
    "\n",
    "# features engineering subset\n",
    "if set(['TotalBsmtSF','1stFlrSF','2ndFlrSF']).issubset(X_feature.columns):\n",
    "    X_feature[\"TotalSF\"] =  X_feature[\"TotalBsmtSF\"] +  X_feature[\"1stFlrSF\"] +  X_feature[\"2ndFlrSF\"]\n",
    "\n",
    "if set(['YearBuilt','YrSold']).issubset(X_feature.columns):\n",
    "    X_feature[\"House_Age\"] = X_feature[\"YrSold\"] - X_feature[\"YearBuilt\"]\n",
    "\n",
    "if set(['YearRemodAdd','YrSold']).issubset(X_feature.columns):\n",
    "    X_feature[\"SinceRemodel\"] = X_feature[\"YrSold\"] - X_feature[\"YearRemodAdd\"]\n",
    "\n",
    "\n",
    "# Bathrooms\n",
    "full = X_feature[\"FullBath\"] if \"FullBath\" in X_feature.columns else 0\n",
    "half = X_feature[\"HalfBath\"] if \"HalfBath\" in X_feature.columns else 0\n",
    "bfull = X_feature[\"BsmtFullBath\"] if \"BsmtFullBath\" in X_feature.columns else 0\n",
    "bhalf = X_feature['BsmtHalfBath'] if 'BsmtHalfBath' in X_feature.columns else 0\n",
    "\n",
    "if isinstance (full, (pd.Series,)):\n",
    "    X_feature[\"TotalBath\"] = full + 0.5*half + bfull + .5*bhalf\n",
    "\n",
    "# Binary Amenities\n",
    "for col in ['PoolArea','GarageArea','TotalBsmtSF','MasVnrArea','Fireplaces']:\n",
    "    if col in X_feature.columns:\n",
    "        X_feature[\"Has_\" + col] = (X_feature[col].fillna(0) > 0).astype(int)\n",
    "print(f\"Engineered features added. New shape: {X_feature.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_feature.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Prepocessing and Model Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize numerical features and categorical Features\n",
    "numerical_cols = X.select_dtypes(include = [np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(exclude = [np.number]).columns.tolist()\n",
    "\n",
    "# Numerical values transformer\n",
    "numeric_transfomer = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy = \"median\"))\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Caegorical values transformer\n",
    "cat_transformer = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the transformers\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transfomer, numerical_cols),\n",
    "        (\"cat\", cat_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define Models\n",
    "ridge_model = Ridge(random_state = RANDOM_STATE)\n",
    "rf_model = RandomForestRegressor(random_state = RANDOM_STATE)\n",
    "gbr_model = GradientBoostingRegressor(random_state = RANDOM_STATE)\n",
    "\n",
    "# Store the models\n",
    "models = {\n",
    "    \"Ridge\": ridge_model,\n",
    "    \"RandomForest\": rf_model,\n",
    "    \"GradientBoosting\": gbr_model\n",
    "}\n",
    "\n",
    "# Evaluation via cross_validation\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# Define cv of 5 folds \n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = RANDOM_STATE)\n",
    "\n",
    "# Evaluate eac model\n",
    "def cv_rsme(model):\n",
    "    pipe = Pipeline(steps = [(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    neg_rsme = cross_val_score(pipe, X_feature, y_log, scoring = \"neg_root_mean_squared_error\", cv = cv, n_jobs = -1)\n",
    "    return -neg_rsme.mean(), -neg_rsme.std()\n",
    "\n",
    "results = {name: cv_rsme(model) for name, model in models.items()}\n",
    "pd.DataFrame(results, index = [\"Rsme_mean(log)\", \"Rsme_std(log)\"]).T.sort_values(\"Rsme_mean(log)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best model by Cv\n",
    "cv_df = pd.DataFrame(results, index = [\"Rsme_mean(log)\", \"Rsme_std(log)\"]).T.sort_values(\"Rsme_mean(log)\")\n",
    "cv_best = cv_df.index[0]\n",
    "print(f\"Best base cv Model:\", {cv_best})\n",
    "\n",
    "# Pick the best model\n",
    "best_model = models[cv_best]\n",
    "\n",
    "# Tune Parameters\n",
    "param_grids = {\n",
    "    \"Ridge\": {\n",
    "        \"model__alpha\": np.logspace(3, 2, 20)\n",
    "    },\n",
    "    \"RandomForest\":{\n",
    "        \"model__n_estimators\": [200,400,800],\n",
    "        \"model__max_depth\": [None, 10, 20, 30],\n",
    "        \"model__min_samples_split\": [2,5,10],\n",
    "        \"model__min_samples_leaf\":[1,2,4]\n",
    "        \n",
    "    },\n",
    "    \"GradientBoosting\":{\n",
    "        \"model__n_estimators\": [200, 400, 600],\n",
    "        \"model__learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "        \"model__max_depth\": [2, 3, 4],\n",
    "        \"model__subsample\": [.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "pipe = Pipeline(steps = [(\"preprocess\",preprocess), (\"model\", best_model)])\n",
    "param_grid = param_grids.get(cv_best, {})\n",
    "\n",
    "if param_grid:\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter = 25,\n",
    "        cv = cv,\n",
    "        scoring = \"neg_root_mean_squared_error\",\n",
    "        n_jobs = -1,\n",
    "        random_state = RANDOM_STATE,\n",
    "        verbose = 1\n",
    "    )\n",
    "    search.fit(X_feature, y_log)\n",
    "    best_pipe = search.best_estimator_\n",
    "    print(f\"Best params: {search.best_params_}\")\n",
    "    print(f\"Best CV Rsme(log): {-search.best_score_}\")\n",
    "\n",
    "else:\n",
    "    print(\"There is no hyperpameters to tune for this model, using base pipeline\")\n",
    "    best_pipe = pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Fit Final Model & Interpret Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "best_pipe.fit(X_feature, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Features names after preprocessing\n",
    "ohe = best_pipe.named_steps[\"preprocess\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "# Extract numerical features\n",
    "num_feat = best_pipe.named_steps[\"preprocess\"].transformers_[0][2]\n",
    "# Etract categorical features\n",
    "cat_feat = ohe.get_feature_names_out(best_pipe.named_steps[\"preprocess\"].transformers_[1][2])\n",
    "# All features \n",
    "feature_names = np.r_[num_feat, cat_feat]\n",
    "\n",
    "# Extract importances(coefficients)\n",
    "model = best_pipe.named_steps[\"model\"]\n",
    "importance = None\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    importance = model.feature_importances_\n",
    "elif hasattr(model, \"coe_\"):\n",
    "    coef = model.coef_.ravel() if hasattr(model._coef_, \"ravel\") else model.coef_\n",
    "    importance = np.abs(coef)\n",
    "\n",
    "else:\n",
    "    print(\"Model does not provide native coefficients\")\n",
    "\n",
    "# Create a dataFrame of the Features ad Importance\n",
    "if importance is not None:\n",
    "    imp_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importance})\n",
    "    imp_df = imp_df.sort_values(\"Importance\", ascending = False).head(25)\n",
    "    imp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature Importance\n",
    "plt.figure(figsize = (8,6))\n",
    "imp_df.plot(kind = \"barh\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature Importance\n",
    "if 'imp_df' in locals():\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(imp_df['Feature'][::-1], imp_df['Importance'][::-1])\n",
    "    # Add a title\n",
    "    plt.title('Top Feature Importances / Coefficient')\n",
    "\n",
    "    # Label Axis\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
